
Car Evaluation.ipynb_

Importing basic Libraries
[4]

import numpy as np

import pandas as pd

​

import matplotlib.pyplot as plt

import seaborn as sns

Importing the Dataset
[5]

%time data = pd.read_csv('drive/My Drive/Projects/practice/Car Evaluation/car_evaluation.csv', header = None)

​

print("Shape of the Data: ", data.shape )

[6]

# Assigning names to the columns in the dataset

​

data.columns = ['Price', 'Maintenance Cost', 'Number of Doors', 'Capacity', 'Size of Luggage Boot', 'safety', 'Decision']

​

Data Insights
[7]

data.head()

[8]

data.sample(5)

[9]

data.tail()

[10]

data.info()

[11]

data.describe()

[12]

data.isnull().any()

[13]

data.columns

Comparative Data Analysis
[14]

price = pd.crosstab(data['Price'], data['Decision'])

price.div(price.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each Price Category', fontsize = 20)

plt.xlabel('Price Range in Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

[15]

mc = pd.crosstab(data['Maintenance Cost'], data['Decision'])

mc.div(mc.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each Maintenance Cost Category', fontsize = 20)

plt.xlabel('Maintenance Cost in Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

[16]

safety = pd.crosstab(data['safety'], data['Decision'])

safety.div(safety.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each Safety Category', fontsize = 20)

plt.xlabel('Safety Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

[17]

doors = pd.crosstab(data['Number of Doors'], data['Decision'])

doors.div(doors.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each doors Category', fontsize = 20)

plt.xlabel('No. of Doors in Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

[18]

capacity = pd.crosstab(data['Capacity'], data['Decision'])

capacity.div(capacity.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each capacity Category', fontsize = 20)

plt.xlabel('Capacity in Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

[19]

luggage = pd.crosstab(data['Size of Luggage Boot'], data['Decision'])

luggage.div(safety.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (10, 7))

​

plt.title('Stacked Bar Graph to Depict portions of Decisions taken on each luggage size Category', fontsize = 20)

plt.xlabel('Luggage Size in Increasing Order', fontsize = 15)

plt.ylabel('Count', fontsize = 15)

plt.legend()

plt.show()

Uni-Variate Data Analysis
[20]

data['Decision'].value_counts().sort_index()

[21]

labels = ['acc', 'good', 'unacc', 'vgood']

colors = ['pink', 'lightblue', 'lightgreen', 'magenta']

size = [384, 69, 1210, 65]

explode = [0.1, 0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (10, 10)

plt.pie(size, labels = labels, colors = colors, explode = explode, shadow = True)

plt.title('A Pie Chart Representing Different Decisions', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[22]

# Label Encoding

​

data.Decision.replace(('unacc', 'acc', 'good', 'vgood'), (0, 1, 2, 3), inplace = True)

​

data['Decision'].value_counts()

[23]

data['Size of Luggage Boot'].value_counts().sort_index()

[24]

labels = ['big', 'medium', 'small',]

colors = ['red', 'cyan', 'orange']

size = [576, 576, 576]

explode = [0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (10, 10)

plt.pie(size, labels = labels, colors = colors, explode = explode, shadow = True)

plt.title('A Pie Chart Representing Different Luggage boot Sizes', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[25]

# label Encoding

​

data['Size of Luggage Boot'].replace(('small', 'med', 'big'), (0, 1, 2), inplace = True)

​

data['Size of Luggage Boot'].value_counts()

[26]

data['safety'].value_counts().sort_index()

[27]

labels = ['high', 'low', 'med']

colors = ['purple', 'cyan', 'lightblue']

size = [576, 576, 576]

explode = [0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (10, 10)

plt.pie(size, labels = labels, colors = colors, explode = explode, shadow = True)

plt.title('A Pie Chart Representing Different Safety Levels', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[28]

# label Encoding

​

data['safety'].replace(('low', 'med', 'high'), (0, 1, 2), inplace = True)

​

data['safety'].value_counts()

[29]

data['Price'].value_counts().sort_index()

[30]

labels = ['high', 'low', 'med', 'vhigh']

colors = ['crimson', 'blue', 'lightpink', 'maroon']

size = [432, 432, 432, 432]

explode = [0.1, 0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (8, 8)

plt.pie(size, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 120, autopct = '%1.1f%%')

plt.title('A Pie Chart Representing Different Price Ranges', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[31]

# label Encoding

​

data['Price'].replace(('low', 'med', 'high', 'vhigh'), (0, 1, 2, 3), inplace = True)

​

data['Price'].value_counts()

[32]

data['Maintenance Cost'].value_counts().sort_index()

[33]

labels = ['high', 'low', 'med', 'vhigh']

colors = ['crimson', 'orange', 'red', 'pink']

size = [432, 432, 432, 432]

explode = [0.1, 0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (8, 8)

plt.pie(size, labels = labels, colors = colors, explode = explode, autopct='%1.1f%%', shadow = True, startangle = 120)

plt.title('A Pie Chart Representing Different Maintenance Cost Ranges', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[34]

# label Encoding

​

data['Maintenance Cost'].replace(('low', 'med', 'high', 'vhigh'), (0, 1, 2, 3), inplace = True)

​

data['Maintenance Cost'].value_counts()

[35]

data['Number of Doors'].value_counts().sort_index()

[36]

labels = ['high', 'low', 'med', 'vhigh']

colors = ['yellow', 'orange', 'green', 'pink']

size = [432, 432, 432, 432]

explode = [0.1, 0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (8, 8)

plt.pie(size, labels = labels, colors = colors, explode = explode, autopct='%1.1f%%', shadow = True, startangle = 120)

plt.title('A Pie Chart Representing Different no. of doors', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[37]

# label Encoding

​

data['Number of Doors'].replace('5more', 5, inplace = True)

​

data['Number of Doors'].value_counts()

[38]

data['Capacity'].value_counts().sort_index()

[39]

labels = ['2', '3', 'more']

colors = ['crimson', 'lightblue', 'lightgreen']

size = [576, 576, 576]

explode = [0.1, 0.1, 0.1]

​

plt.rcParams['figure.figsize'] = (8, 8)

plt.pie(size, labels = labels, colors = colors, explode = explode, autopct='%1.1f%%', shadow = True, startangle = 120)

plt.title('A Pie Chart Representing Different Capacities', fontsize = 20)

plt.axis('off')

plt.legend()

plt.show()

[40]

# label Encoding

​

data['Capacity'].replace('more', 5, inplace = True)

​

data['Capacity'].value_counts()

[41]

data.sample(5)

BI-Variate Data Analysis
[42]

plt.rcParams['figure.figsize'] = (12, 8)

ax = sns.violinplot(x = data['Price'], y = data['Decision'], color = 'g')

ax.set_title('Violin Plot to show relation between Price and Decision', fontsize = 20)

ax.set_xlabel('Price in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

[43]

​

sns.set(style = 'dark', palette = 'colorblind', color_codes = True)

plt.rcParams['figure.figsize'] = (12, 8)

​

ax = sns.violinplot(x = data['Maintenance Cost'], y = data['Decision'], color = 'y')

ax.set_title('Violin Plot to show relation between Maintenance Cost and Decision', fontsize = 20)

ax.set_xlabel('Maintenance Cost in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

[44]

​

sns.set(style = 'dark', palette = 'muted', color_codes = True)

plt.rcParams['figure.figsize'] = (12, 8)

​

ax = sns.violinplot(x = data['Number of Doors'], y = data['Decision'], color = 'r')

ax.set_title('Violin Plot to show relation between Number of Doors and Decision', fontsize = 20)

ax.set_xlabel('Number of Doors in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

[45]

​

sns.set(style = 'dark', palette = 'colorblind', color_codes = True)

plt.rcParams['figure.figsize'] = (12, 8)

​

ax = sns.violinplot(x = data['Capacity'], y = data['Decision'], color = 'b')

ax.set_title('Violin Plot to show relation between Capacity and Decision', fontsize = 20)

ax.set_xlabel('Capcity in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

[46]

​

sns.set(style = 'dark', palette = 'colorblind', color_codes = True)

plt.rcParams['figure.figsize'] = (12, 8)

​

ax = sns.violinplot(x = data['Size of Luggage Boot'], y = data['Decision'], color = 'm')

ax.set_title('Violin Plot to show relation between Size of Luggage Boot and Decision', fontsize = 20)

ax.set_xlabel('Size of Luggage Boot in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

[47]

​

sns.set(style = 'dark', palette = 'colorblind', color_codes = True)

plt.rcParams['figure.figsize'] = (12, 8)

​

ax = sns.boxplot(x = data['safety'], y = data['Decision'], color = 'w')

ax.set_title('Box Plot to show relation between safety and Decision', fontsize = 20)

ax.set_xlabel('safety in Increasing range', fontsize = 15)

ax.set_ylabel('Decision in Positive vibe')

plt.show()

Data PreProcessing
[48]

data.shape

[49]

# splitting the dataset into dependent and independent variables

​

x = data.iloc[:,:6]

y = data.iloc[:, 6]

​

print("Shape of x: ", x.shape)

print("Shape of y: ", y.shape)

[50]

# Label Encoding once more so that we get higher accuracy

# we have 4 classes namely 0, 1, 2, 3

# It would be very beneficial if we combine 0 and 1 as 0 and 1 and 2 as 1

​

data.Decision.replace((0, 1, 2, 3), (0, 0, 1, 1), inplace = True)

​

data['Decision'].value_counts()

[51]

# splitting the dataset into train and test sets

​

from sklearn.model_selection import train_test_split

​

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.15, random_state = 0)

​

print("Shape of x_train: ", x_train.shape)

print("Shape of y_train: ", y_train.shape)

print("shape of x_test: ", x_test.shape)

print("shape of y_test: ", y_test.shape)

[52]

# standardization

​

from sklearn.preprocessing import StandardScaler

​

sc = StandardScaler()

​

x_train = sc.fit_transform(x_train)

x_test = sc.transform(x_test)

​

Modelling

Logistic Regression
[53]

#importing libraries

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import confusion_matrix

​

# creating a model

model = LogisticRegression(C = 1)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Trees with max_depth = 3
[54]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 3)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Tree with max_depth = 2
[55]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 2)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

​

Decision Tree with max_depth = 4
[56]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 4)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Trees with max depth 5
[57]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 5)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Tree with max depth = 6
[58]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 6)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Tree with max depth = 7
[59]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 7)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Tree with max depth = 8
[60]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 8)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decision Tree with max depth = 9
[61]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 9)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

Decsision Tree with max depth =10
[62]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier(max_depth = 10)

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

using default value of max_depth We are getting the best result of 98.07% accuracy over the test set
[63]

#importing libraries

from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = DecisionTreeClassifier()

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[87]

#  plotting the graph for performance of decision trees with different max_depth values

​

max_depth = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10])

Training_Accuracy = np.array([93.4, 93.5, 97.3, 97.3, 94.6, 97.3, 97.3, 98.4, 98.07])

Testing_Accuracy = np.array([92.02, 92.02, 95.9, 95.9, 97.8, 98.4, 99.3, 99.7, 100.0])

​

plt.rcParams['figure.figsize'] = (10, 7)

plt.plot(max_depth, Training_Accuracy, 'r--')

plt.plot(max_depth, Testing_Accuracy, 'b-*')

plt.title('Performance of Decision Tree with Different Values of max depth', fontsize = 20)

plt.xlim([1, 11])

plt.ylim([90, 100])

plt.xlabel('Max Depth ', fontsize = 15)

plt.ylabel('Accuracy', fontsize = 15)

plt.legend()

plt.show()

Random Forest
[65]

#importing libraries

from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import confusion_matrix

​

# creating a model

model = RandomForestClassifier()

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[66]

# checking the feature importance

​

imp = model.feature_importances_

​

print(imp)

​

[67]

import matplotlib.pyplot as plt

import seaborn as sns

​

Acc = np.array([97.6, 98.4, 99.2])

count = np.array([1, 2, 3])

plt.scatter(count, Acc, color = 'green')

plt.title('1 Represents Logistic Regression, 2 Represents Decision Tree and 3 Represents Random Forest \

It plots a graph to compare performance of these Classifiers', fontsize = 20)

plt.xlabel('Classifiers', fontsize = 15)

plt.ylabel('Accuracy', fontsize = 15)

​

plt.show()

CONCLUSSION: Logistic Regression is producing an accuracy of 97.6%, whereas Decision Tree produced 98.07% accuracy and Random Forest Produced 99.2% accuracy which is the highest of all and all the model are performing best with their default values as it is completely processed dataset italicized text

KNN Classifiers
[71]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 1)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[73]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 2)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[75]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 3)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[76]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 4)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[78]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 5)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[79]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 6)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[80]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 7)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[81]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 8)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[82]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 9)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

[83]

from sklearn.neighbors import KNeighborsClassifier

​

# creating a model

model = KNeighborsClassifier(n_neighbors = 10)

​

# feeding the training data into the model

model.fit(x_train, y_train)

​

# predicting the values for x-test

y_pred = model.predict(x_test)

​

# finding the training and testing accuracy

print("Training Accuracy: ",model.score(x_train, y_train))

print("Testing Accuracy: ", model.score(x_test, y_test))

​

# printing the confusion Matrix

cm = confusion_matrix(y_test, y_pred)

print(cm)

#  plotting the graph for performance of decision trees with different max_depth values

​

max_depth = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

Training_Accuracy = np.array([100, 98.9, 99.7, 99.7, 99.8, 99.1, 99.5, 99.2, 99.3, 99.2])

Testing_Accuracy = np.array([98.8, 96.9, 99.2, 98.4, 98.8, 97.6, 98.8, 98.8, 98.8, 98])

​

plt.rcParams['figure.figsize'] = (10, 7)

plt.plot(max_depth, Training_Accuracy, 'y--')

plt.plot(max_depth, Testing_Accuracy, 'g-*')

plt.title('Performance of KNN Classifiers with Different Values of neighbors', fontsize = 20)

plt.xlim([0, 11])

plt.ylim([95, 100])

plt.xlabel('Max Depth ', fontsize = 15)

plt.ylabel('Accuracy', fontsize = 15)

plt.legend()

plt.show()

Let's evaluate some real life examples and check what it is predicting
[ ]

data.columns

[ ]

data.head()

[ ]

'''

predicting if the costumer's Decision for buying a car with following specifications will be good or not

​

Price : 2

Maintenance Cost = 1

Number of Doors = 2

Capacity = 3

Size of Luggage Boot = 1

safety = 2

​

'''

​

[ ]

new_prediction = model.predict(sc.transform(np.array([[2, 1, 2, 3, 1, 2]])))

​

new_prediction = (new_prediction > 0.5 )

print(new_prediction)

[ ]

'''

predicting if the costumer's Decision for buying a car with following specifications will be good or not

​

Price : 1

Maintenance Cost = 1

Number of Doors = 3

Capacity = 3

Size of Luggage Boot = 2

safety = 3

​

'''

[ ]

new_prediction = model.predict(sc.transform(np.array([[1, 1, 3, 3, 2, 3]])))

​

new_prediction = (new_prediction > 0.5 )

print(new_prediction)

Saved successfully!
